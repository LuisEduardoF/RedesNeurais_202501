{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOE2WTDAkagT"
   },
   "source": [
    "# üß†ü§ñ Treinamento de Redes LSTM para Classifica√ß√£o\n",
    "\n",
    "- **Deadline**: 24/08/2025\n",
    "- **Entrega**: O trabalho deve ser entregue via sistema Testr.\n",
    "- **Pontua√ß√£o**: 50% da nota do T2 (+1 ponto extra).\n",
    "- O trabalho deve ser realizado individualmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU05mfhsQB6Y"
   },
   "source": [
    "## Especifica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdAdEyR69fd1"
   },
   "source": [
    "\n",
    "### Contexto\n",
    "\n",
    "O trabalho consiste em realizar o treinamento de redes LSTM usando a base de dados [BBC News Archive dispon√≠vel no kaggle](https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive?select=bbc-news-data.csv). Esta base de dados cont√©m 2.225 textos publicados no site de not√≠cias da BBC news entre 2004-2005. Cada not√≠cia foi classificada como sendo de um dos seguintes assuntos: business (neg√≥cios), entertainment (entretenimento), politics (pol√≠tica), sport (esportes), tech (tecnologia).\n",
    "\n",
    "O objetivo do trabalho √© treinar uma rede neural capaz de identificar o tema de um texto. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementa√ß√£o \n",
    "\n",
    "- Use o notebook de classifica√ß√£o de sentimentos como ponto de partida.\n",
    "- use a biblioteca `kagglehub` para fazer o download do dataset no colab.\n",
    "- Um dos modelos de *word embeddings* dispon√≠veis na biblioteca `gensim` deve ser utilizado para mapear palavras em vetores. \n",
    "- Use o tipo `nn.LSTM` dispon√≠vel no `pytorch` (n√£o √© necess√°rio implementar a camada LSTM do zero).\n",
    "- Os dados devem ser divididos em treino, valida√ß√£o e teste. Use o conjunto de valida√ß√£o para ajustar hiperpar√¢metros e para selecionar o modelo com melhor generaliza√ß√£o. Avalie o modelo resultante usando o conjunto de teste apenas ao final. \n",
    "- Voc√™ pode optar por cortar os textos em um tamanho m√°ximo (e.g., 100 palavras), como fizemos no notebook, para que os testes n√£o demorem muito.\n",
    "- Use o ambiente de `GPU` do colab para evitar que o treinamento demore excessivamente.\n",
    "- Durante o desenvolvimento, √© uma boa id√©ia usar um subconjunto (e.g., 10%) das not√≠cias para que os testes sejam mais r√°pidos. Quando tudo estiver correto, fa√ßa o treinamento com a base completa.\n",
    "- Deve ser plotado o gr√°fico mostrando a evolu√ß√£o da fun√ß√£o de perda nos conjuntos de treino e valida√ß√£o. \n",
    "- Devem ser mostradas as m√©tricas geradas pela fun√ß√£o `classification_report` da biblioteca scikit-learn e a matriz de confus√£o para o conjunto de teste. \n",
    "- Fa√ßa alguns testes qualitativos com textos escritos com voc√™ (n√£o use textos da base de dados).\n",
    "- Discuta brevemente os resultados quantitativos e qualitativos (1-2 par√°grafos, no m√°ximo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccJn9-T_Ts6e"
   },
   "source": [
    "\n",
    "### Pontos Extras\n",
    "\n",
    "Receber√° um ponto extra, o aluno que:\n",
    "- Utilizar um LLM baseado em Transformer pr√©-treinado (e.g., [BERT](https://medium.com/@davidlfliang/intro-getting-started-with-text-embeddings-using-bert-9f8c3b98dee6)) para mapear as not√≠cias em *embeddings*.\n",
    "- Utilizar uma rede Multilayer Perceptron para classificar os *embeddings*. \n",
    "- Comparar a performance desta solu√ß√£o com a LSTM. \n",
    "\n",
    "‚ö†Ô∏è**IMPORTANTE**‚ö†Ô∏è\n",
    "- N√£o √© necess√°rio (nem recomend√°vel considerando o prazo) tentar realizar *fine-tuning* do LLM pr√©-treinado.\n",
    "- Estes modelos s√£o SUPER-ULTRA-MASTER-BLASTER lentos na CPU. Use o ambiente de GPU do colab para evitar ficar 20h esperando para transformar os textos em *embeddings*.\n",
    "- Salve os embeddings depois da gera√ß√£o para evitar ter que ger√°-los novamente. Quando necess√°rio, fa√ßa upload do arquivo novamente para o colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de bibliotecas necess√°rias (executar apenas uma vez)\n",
    "!pip install -q kagglehub gensim torch torchvision torchaudio sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import downloader as api\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do dataset BBC News Archive usando kagglehub\n",
    "# Ser√° feito download e extra√ß√£o do arquivo CSV. Pode demorar alguns minutos.\n",
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.download_dataset(\"hgultekin/bbcnewsarchive\", download_method=\"http\", force_download=False)\n",
    "\n",
    "# Carregar o arquivo CSV em um DataFrame\n",
    "import os\n",
    "csv_path = None\n",
    "# Procura pelo arquivo CSV dentro do diret√≥rio baixado\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for fname in files:\n",
    "        if fname.endswith('.csv'):\n",
    "            csv_path = os.path.join(root, fname)\n",
    "            break\n",
    "    if csv_path:\n",
    "        break\n",
    "\n",
    "assert csv_path is not None, \"Arquivo CSV n√£o encontrado no dataset\"\n",
    "\n",
    "# Carrega o DataFrame\n",
    "bbc_df = pd.read_csv(csv_path)\n",
    "print(bbc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de limpeza de texto: remove pontua√ß√µes, coloca em min√∫sculas e quebra em tokens\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove pontua√ß√£o\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove d√≠gitos\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Aplica limpeza a todas as not√≠cias\n",
    "bbc_df['tokens'] = bbc_df['content'].apply(clean_text)\n",
    "\n",
    "# Codifica r√≥tulos (categorias) como inteiros\n",
    "label_encoder = LabelEncoder()\n",
    "bbc_df['label_id'] = label_encoder.fit_transform(bbc_df['category'])\n",
    "\n",
    "# Divide o conjunto em treino, valida√ß√£o e teste\n",
    "train_df, test_df = train_test_split(bbc_df, test_size=0.2, random_state=42, stratify=bbc_df['label_id'])\n",
    "train_df, val_df  = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label_id'])\n",
    "\n",
    "print(f\"Total de amostras: {len(bbc_df)}, Treino: {len(train_df)}, Valida√ß√£o: {len(val_df)}, Teste: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega embeddings pr√©-treinados do Gensim.\n",
    "# Existem diversos modelos dispon√≠veis; aqui usamos glove-wiki-gigaword-100 (100 dimens√µes).\n",
    "embedding_model = api.load('glove-wiki-gigaword-100')\n",
    "embedding_dim = embedding_model.vector_size\n",
    "\n",
    "# Constroi um vocabul√°rio apenas com as palavras presentes nos dados de treino e existentes no modelo de embeddings\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "embedding_matrix = [np.zeros(embedding_dim), np.zeros(embedding_dim)]  # √≠ndices 0 e 1 para PAD e UNK\n",
    "\n",
    "for tokens in train_df['tokens']:\n",
    "    for token in tokens:\n",
    "        if token not in vocab and token in embedding_model:\n",
    "            vocab[token] = len(vocab)\n",
    "            embedding_matrix.append(embedding_model[token])\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "print(f\"Tamanho do vocabul√°rio: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f30417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento m√°ximo das sequ√™ncias (trunca ou faz padding para esse tamanho)\n",
    "max_seq_length = 100\n",
    "\n",
    "# Fun√ß√£o que converte uma lista de tokens em √≠ndices do vocabul√°rio\n",
    "def tokens_to_indices(tokens, vocab, max_len):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            indices.append(vocab[token])\n",
    "        else:\n",
    "            indices.append(vocab['<UNK>'])\n",
    "    # Truncar ou fazer padding\n",
    "    if len(indices) < max_len:\n",
    "        indices += [vocab['<PAD>']] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    return indices\n",
    "\n",
    "# Dataset personalizado\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df.loc[idx, 'tokens']\n",
    "        label = self.df.loc[idx, 'label_id']\n",
    "        indices = tokens_to_indices(tokens, self.vocab, self.max_len)\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Cria inst√¢ncias dos datasets e DataLoaders\n",
    "train_dataset = NewsDataset(train_df, vocab, max_seq_length)\n",
    "val_dataset   = NewsDataset(val_df, vocab, max_seq_length)\n",
    "test_dataset  = NewsDataset(test_df, vocab, max_seq_length)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers=1, bidirectional=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        # Carrega pesos pr√©-treinados e congela para n√£o treinar\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32), requires_grad=False)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers, batch_first=True,\n",
    "                            bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        # Pega o √∫ltimo estado oculto\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        output = self.dropout(hidden)\n",
    "        out = self.fc(output)\n",
    "        return out\n",
    "\n",
    "# Hiperpar√¢metros do modelo\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_encoder.classes_)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers, bidirectional, dropout=0.5)\n",
    "\n",
    "# Define crit√©rio de perda e otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Move para GPU se dispon√≠vel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        epoch_acc += (preds == labels).sum().item()\n",
    "        total += inputs.size(0)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            epoch_acc += (preds == labels).sum().item()\n",
    "            total += inputs.size(0)\n",
    "    return epoch_loss / total, epoch_acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Treino Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "    # Salva o melhor modelo com base na acur√°cia de valida√ß√£o\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pt')\n",
    "        print(\"\n",
    "Melhor modelo salvo com acur√°cia de valida√ß√£o:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o melhor modelo salvo\n",
    "test_model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers, bidirectional, dropout=0.5)\n",
    "test_model.load_state_dict(torch.load('best_lstm_model.pt'))\n",
    "test_model = test_model.to(device)\n",
    "\n",
    "# Avalia no conjunto de teste\n",
    "test_loss, test_acc = evaluate(test_model, test_loader, criterion)\n",
    "print(f\"Desempenho no conjunto de teste - Loss: {test_loss:.4f}, Acur√°cia: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
